{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "A2YW1svvqjvG",
        "KhL_pxHx7i07",
        "KUK2H8o_Rv5s",
        "RHt-aUf0-Vjq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "3H96kPkzqf0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ale-py\n",
        "!pip install gymnasium"
      ],
      "metadata": {
        "id": "9dU_Oy9lqnQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import ale_py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "7VbB9_hdqhkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partie 1: Frozen Lake"
      ],
      "metadata": {
        "id": "A2YW1svvqjvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons commencer notre TP avec un environnement type grille: le [Frozen Lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake). Cela vous aidera également à vous familiariser avec l'API Gym, largement utilisée de le monde du RL.\n",
        "\n",
        "Commencez par lire la [documentation](https://gymnasium.farama.org/environments/toy_text/frozen_lake) de l'environnement Frozen Lake."
      ],
      "metadata": {
        "id": "tOQWOl2XweTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print utils"
      ],
      "metadata": {
        "id": "6IgXHaBg4p-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def int_action_to_str(int_action):\n",
        "    if int_action == 0:\n",
        "        return \"left\"\n",
        "    if int_action == 1:\n",
        "        return \"down\"\n",
        "    if int_action == 2:\n",
        "        return \"right\"\n",
        "    else:\n",
        "      return \"up\""
      ],
      "metadata": {
        "id": "yRonTcLQ4scp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_values(values, grid_width):\n",
        "    for i in range(int(len(values)/grid_width)):\n",
        "        print(values[i*grid_width:(i+1)*grid_width])\n",
        "\n",
        "def print_policy(policy, grid_width):\n",
        "    for i in range(int(len(policy)/grid_width)):\n",
        "        print([int_action_to_str(_a) for _a in policy[i*grid_width:(i+1)*grid_width]])"
      ],
      "metadata": {
        "id": "gx6ZY5XS3bpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_q_values(q_values, grid_width):\n",
        "    for i in range(int(len(q_values)/grid_width)):\n",
        "        _q_values_to_print = []\n",
        "        for _q_values in q_values[i*grid_width:(i+1)*grid_width]:\n",
        "            _q_values_to_print.append({int_action_to_str(_k): _v for _k, _v in enumerate(_q_values)})\n",
        "        print(_q_values_to_print)"
      ],
      "metadata": {
        "id": "L46vWeKZ5j0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A) Version déterministe"
      ],
      "metadata": {
        "id": "Qu_jZtNFw5FS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Créons une première instance de l'environnement avec une carte spécifique:"
      ],
      "metadata": {
        "id": "LZV144o4ynmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1', desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"], is_slippery=False, render_mode=\"rgb_array\")"
      ],
      "metadata": {
        "id": "wTqEgIM_vPn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il est nécessaire de reset l'environnement pour lancer un épisode:"
      ],
      "metadata": {
        "id": "3zpIjUdRyt1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "observation, info = env.reset()"
      ],
      "metadata": {
        "id": "FffA5N7WvNaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "id": "BhUo2TnTvaVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regardez la fonction de transition:"
      ],
      "metadata": {
        "id": "2cYGke80yzjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_transition_function(env, state, action, ):\n",
        "    print(f\"From state {state} when playing action {action}:\")\n",
        "    for next_state_transition in env.unwrapped.P[state][action]:\n",
        "        print(f\"- Reaching state {next_state_transition[1]} along with reward {next_state_transition[2]} with probability {next_state_transition[0]}\")"
      ],
      "metadata": {
        "id": "NDjtbUgAw_ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# State: initial\n",
        "# Action: right\n",
        "print_transition_function(env, state=0, action=2)"
      ],
      "metadata": {
        "id": "_mV1Xav4w24l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# State: cell 4 (next ot a hole)\n",
        "# Action: right\n",
        "print_transition_function(env, state=4, action=2)"
      ],
      "metadata": {
        "id": "tUz6TL6bzBmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# State: cell 14 (next to the goal)\n",
        "# Action: right\n",
        "print_transition_function(env, state=14, action=2)"
      ],
      "metadata": {
        "id": "01NvxPuNznHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Codez désormais une stratégie permettant de résoudre le maze:"
      ],
      "metadata": {
        "id": "CPAwykBF0PSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "observation, info = env.reset()\n",
        "done, truncated = False, False\n",
        "reward = 0\n",
        "while not (done or truncated):\n",
        "    action = <your code>\n",
        "    observation, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "assert reward == 1"
      ],
      "metadata": {
        "id": "zZVkOQvA0Uvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testez à nouveau votre stratégie avec cette nouvelle carte. Que fait-elle ?"
      ],
      "metadata": {
        "id": "hmO-4tyZ11aU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1', desc=[\"SHFF\", \"FHFH\", \"FFHF\", \"FFFG\"], is_slippery=False, render_mode=\"rgb_array\")\n",
        "observation, info = env.reset()\n",
        "done, truncated = False, False\n",
        "reward = 0\n",
        "while not (done or truncated):\n",
        "    action = <your code>\n",
        "    observation, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "assert reward == 1"
      ],
      "metadata": {
        "id": "Kk0iWZsb1fku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B) Version stochastique"
      ],
      "metadata": {
        "id": "FaxHKJz23WiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passons maintenant à une verison sotchastique où notre agent peut \"glisser\"."
      ],
      "metadata": {
        "id": "t3wORcs-3ohn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1', desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"], is_slippery=True, render_mode=\"rgb_array\")"
      ],
      "metadata": {
        "id": "A1jI-f_Q2Srq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "observation, info = env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "id": "5xyL4WDm3tyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regardons ce que cela change sur la fonction de transition:"
      ],
      "metadata": {
        "id": "WKhcRQID305z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# State: initial\n",
        "# Action: right\n",
        "print_transition_function(env, state=0, action=2)"
      ],
      "metadata": {
        "id": "oI32zkVy36eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# State: cell 4 (next ot a hole)\n",
        "# Action: right\n",
        "print_transition_function(env, state=4, action=2)"
      ],
      "metadata": {
        "id": "xogdKNsG36eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# State: cell 14 (next to the goal)\n",
        "# Action: right\n",
        "print_transition_function(env, state=14, action=2)"
      ],
      "metadata": {
        "id": "WQwMvfTj36eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essayez maintenant de trouver une stratégie sûre pour la carte suivante:"
      ],
      "metadata": {
        "id": "3ConoxBZ4HMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1', desc=[\"SFFF\", \"FFFF\", \"FFFF\", \"HFFG\"], is_slippery=True, render_mode=\"rgb_array\")\n",
        "observation, info = env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "id": "AC8vN2VL4br_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_transition_function(env, state=8, action=1)"
      ],
      "metadata": {
        "id": "SWtSJSbH5kmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Votre stratégie sera testée plusieurs fois pour s'assurer de sa fiabilité."
      ],
      "metadata": {
        "id": "SioJQuoT4v2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  observation, info = env.reset()\n",
        "  done, truncated = False, False\n",
        "  reward = 0\n",
        "  while not (done or truncated):\n",
        "      action = <your code>\n",
        "      observation, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "  assert reward == 1"
      ],
      "metadata": {
        "id": "y60aCxFw33km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partie 2: Dynamic Programming"
      ],
      "metadata": {
        "id": "KhL_pxHx7i07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1', desc=[\"SFF\", \"FFF\", \"FFG\"], is_slippery=False, render_mode=\"rgb_array\")\n",
        "observation, info = env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "id": "Vunw7_V2IEdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A) Policy Iteration"
      ],
      "metadata": {
        "id": "CRAKpUFKQBwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Commençons par implémenter la méthode de Policy Iteration:"
      ],
      "metadata": {
        "id": "_zgrdPcHP9jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(env, gamma=0.99):\n",
        "    values = np.zeros(env.observation_space.n)\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int16)\n",
        "\n",
        "    values_list = [values.copy()]\n",
        "    policy_list = [policy.copy()]\n",
        "\n",
        "    converged = False\n",
        "    actions = list(range(env.action_space.n))\n",
        "    states = list(env.unwrapped.P.keys())\n",
        "    rewards = {}\n",
        "    terminal_states = []\n",
        "    for s in env.unwrapped.P:\n",
        "        for a in env.unwrapped.P[s]:\n",
        "            for s_prime in env.unwrapped.P[s][a]:\n",
        "                if s_prime[1] not in rewards:\n",
        "                  rewards[s_prime[1]] = s_prime[2]\n",
        "                if s_prime[3] and s_prime[1] not in terminal_states:\n",
        "                    terminal_states.append(s_prime[1])\n",
        "\n",
        "    while not converged:\n",
        "        # Step 1 : Policy Evaluation\n",
        "        for s in states:\n",
        "            if s in terminal_states:\n",
        "                values[s] = rewards[s]\n",
        "            else :\n",
        "                current_policy_action = policy[s]\n",
        "                sum = 0\n",
        "                for s_prime in env.unwrapped.P[s][current_policy_action]:\n",
        "                    sum = sum +  s_prime[0]*values[s_prime[1]]\n",
        "                values[s] = rewards[s] + gamma*sum\n",
        "\n",
        "\n",
        "        # Step 2 : Policy Improvement\n",
        "        for s in states:\n",
        "            _values = []\n",
        "            for a in actions:\n",
        "                sum = 0\n",
        "                for s_prime in env.unwrapped.P[s][a]:\n",
        "                    sum = sum + s_prime[0]*values[s_prime[1]]\n",
        "                _values.append(rewards[s] + gamma*sum)\n",
        "            policy[s] = np.argmax(_values)\n",
        "\n",
        "        values_list.append(values.copy())\n",
        "        policy_list.append(policy.copy())\n",
        "\n",
        "        # Check the convergence\n",
        "        if np.array_equal(values_list[-1],values_list[-2]) :\n",
        "            converged = True\n",
        "\n",
        "    return [values_list, policy_list]"
      ],
      "metadata": {
        "id": "XaxSnAsS78Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values_list, policy_list = policy_iteration(env)\n",
        "for _values, _policy in zip(values_list, policy_list):\n",
        "    print(\"Values:\")\n",
        "    print_values(_values, grid_width=3)\n",
        "    print(\"Policy:\")\n",
        "    print_policy(_policy, grid_width=3)"
      ],
      "metadata": {
        "id": "KneeXKYHGNIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B) Value Iteration"
      ],
      "metadata": {
        "id": "WxkPFg6RQH-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A votre tour pour la value iteration !"
      ],
      "metadata": {
        "id": "LZQbjyWxQMVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(env, gamma=0.99):\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int16)\n",
        "    values = np.zeros(env.observation_space.n)\n",
        "    values_list = [values.copy()]\n",
        "\n",
        "    converged = False\n",
        "    actions = list(range(env.action_space.n))\n",
        "    states = list(env.unwrapped.P.keys())\n",
        "    rewards = {}\n",
        "    terminal_states = []\n",
        "    for s in env.unwrapped.P:\n",
        "        for a in env.unwrapped.P[s]:\n",
        "            for s_prime in env.unwrapped.P[s][a]:\n",
        "                if s_prime[1] not in rewards:\n",
        "                  rewards[s_prime[1]] = s_prime[2]\n",
        "                if s_prime[3] and s_prime[1] not in terminal_states:\n",
        "                    terminal_states.append(s_prime[1])\n",
        "\n",
        "    while not converged:\n",
        "        # Step 1 : Greedy Policy Evaluation\n",
        "        <your code>\n",
        "        values_list.append(values.copy())\n",
        "\n",
        "        # Check the convergence\n",
        "        if np.array_equal(values_list[-1],values_list[-2]) :\n",
        "            converged = True\n",
        "\n",
        "    # Final step: construct the policy\n",
        "    <your code>\n",
        "\n",
        "    return [values_list, policy]"
      ],
      "metadata": {
        "id": "cmFrmezDQJVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values_list, policy = value_iteration(env)\n",
        "for _values in values_list:\n",
        "    print(\"Values:\")\n",
        "    print_values(_values, grid_width=3)\n",
        "\n",
        "print(\"Policy:\")\n",
        "print_policy(policy, grid_width=3)"
      ],
      "metadata": {
        "id": "rIHIiOr3Q3xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partie 3: Q-Learning"
      ],
      "metadata": {
        "id": "KUK2H8o_Rv5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A) No Holes"
      ],
      "metadata": {
        "id": "RHt-aUf0-Vjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1', desc=[\"SFF\", \"FFF\", \"FFG\"], is_slippery=False, render_mode=\"rgb_array\")\n",
        "observation, info = env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "id": "YBprdZAxfD_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa(env, num_episodes=2000, epsilon=0.1, gamma=0.99, alpha=0.8, reward_on_hole=0, grid_width=3):\n",
        "    q_table = np.zeros((env.observation_space.n, env.action_space.n), dtype=np.float32)\n",
        "    for i in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "        done, truncated = False, False\n",
        "        _return = 0\n",
        "        # Sample the first action\n",
        "        if np.random.rand(1) < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(q_table[next_state, :])\n",
        "        while not (done or truncated):\n",
        "            # Play the action\n",
        "            next_state, reward, done, truncated, info = env.step(action)\n",
        "            _return += reward\n",
        "\n",
        "            # Sample the next action\n",
        "            if np.random.rand(1) < epsilon:\n",
        "                next_action = env.action_space.sample()\n",
        "            else:\n",
        "                next_action = np.argmax(q_table[next_state, :])\n",
        "\n",
        "            # Update Q-Table with new knowledge\n",
        "            if not (done or truncated):\n",
        "                td_error = <your code>\n",
        "                q_table[state, action] = <your code>\n",
        "                state = next_state\n",
        "                action = next_action\n",
        "            else:\n",
        "                if reward == 0 and not truncated:  # Fell in a hole\n",
        "                    reward = reward_on_hole\n",
        "                td_error = <your code>\n",
        "                q_table[state, action] = <your code>\n",
        "\n",
        "        print(f\"Obtained return {_return} at episode {i}\")\n",
        "\n",
        "    print_q_values(q_table, grid_width=grid_width)"
      ],
      "metadata": {
        "id": "13Et6OFVRyGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sarsa(env, num_episodes=2000, epsilon=0.1, grid_width=3)"
      ],
      "metadata": {
        "id": "943h4IMSb-85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(env, num_episodes=2000, epsilon=0.1, gamma=0.99, alpha=0.8, reward_on_hole=0, grid_width=3):\n",
        "    q_table = np.zeros((env.observation_space.n, env.action_space.n), dtype=np.float32)\n",
        "    for i in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "        done, truncated = False, False\n",
        "        _return = 0\n",
        "        # Sample the first action\n",
        "        if np.random.rand(1) < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(q_table[next_state, :])\n",
        "        while not (done or truncated):\n",
        "            # Play the action\n",
        "            next_state, reward, done, truncated, info = env.step(action)\n",
        "            _return += reward\n",
        "\n",
        "            # Sample the next action\n",
        "            if np.random.rand(1) < epsilon:\n",
        "                next_action = env.action_space.sample()\n",
        "            else:\n",
        "                next_action = np.argmax(q_table[next_state, :])\n",
        "\n",
        "            # Update Q-Table with new knowledge\n",
        "            if not (done or truncated):\n",
        "                td_error = <your code>\n",
        "                q_table[state, action] = <your code>\n",
        "                state = next_state\n",
        "                action = next_action\n",
        "            else:\n",
        "                if reward == 0 and not truncated:  # Fell in a hole\n",
        "                    reward = reward_on_hole\n",
        "                td_error = <your code>\n",
        "                q_table[state, action] = <your code>\n",
        "\n",
        "        print(f\"Obtained return {_return} at episode {i}\")\n",
        "\n",
        "    print_q_values(q_table, grid_width=grid_width)"
      ],
      "metadata": {
        "id": "vEDz9cR5d2mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_learning(env, num_episodes=2000, epsilon=0.1, grid_width=3)"
      ],
      "metadata": {
        "id": "GyPbIPZkd2mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B) Holes"
      ],
      "metadata": {
        "id": "8tLxoL5f0Xqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essayons maintenant de reproduire l'exemple \"The Cliff\" du cours."
      ],
      "metadata": {
        "id": "j2gaAaYa93_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1', desc=[\"FFFFF\", \"FFFFF\", \"SHHHG\"], is_slippery=False, render_mode=\"rgb_array\")\n",
        "observation, info = env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "id": "FSjwi_rT0e2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sarsa(env, num_episodes=20000, epsilon=0.33, reward_on_hole=-1, grid_width=5)"
      ],
      "metadata": {
        "id": "vVK7VO4h0tAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_learning(env, num_episodes=20000, epsilon=0.33, reward_on_hole=-1, grid_width=5)"
      ],
      "metadata": {
        "id": "iYZk1oFT0skd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partie 4: DQN"
      ],
      "metadata": {
        "id": "A80Ac3AoR0vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il est temps de passer à un environnnement plus compliqué: Pong."
      ],
      "metadata": {
        "id": "r9FoP3reMAXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pong = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
        "print(\"Observation space:\", pong.observation_space)\n",
        "print(\"Action space:\", pong.action_space)\n",
        "print(f\"Actions' meaning: {pong.unwrapped.get_action_meanings()}\")\n",
        "s,_ = pong.reset()\n",
        "plt.imshow(pong.render(),vmin=0,vmax=255);"
      ],
      "metadata": {
        "id": "2_LrPp4IA1T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons utiliser des wrappers déjà fournis par Gymnasium pour:\n",
        "- processer l'image (réduire sa taille, tout mettre en gris...)\n",
        "- stacker les N dernières images (permettant de tracker le mouvement des objets)"
      ],
      "metadata": {
        "id": "lb07NvacMF4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation"
      ],
      "metadata": {
        "id": "0eEcTaEzDKF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pong = AtariPreprocessing(pong)\n",
        "pong = FrameStackObservation(pong, 4)"
      ],
      "metadata": {
        "id": "WK8p_dE9DLja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying a random agent in Pong\n",
        "import time\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "_,_ = pong.reset()\n",
        "for i in range(10):\n",
        "    a = np.random.randint(2)\n",
        "    x, r, _, _, _ = pong.step(a)\n",
        "print(\"shape: \", x.shape, \", min = \", x.min(), \", max = \", x.max(), \", dtype = \", x.dtype, sep='')\n",
        "plt.imshow(x[0], cmap='gray')"
      ],
      "metadata": {
        "id": "jvZDu9puDQNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A) Replay Buffer"
      ],
      "metadata": {
        "id": "ycHzHli5Mc4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implémentons ensuite notre replay buffer permettant de sampler aléatoirement des transitions collectées."
      ],
      "metadata": {
        "id": "gZ0xccD_Mfoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity, device):\n",
        "        self.capacity = capacity # capacity of the buffer\n",
        "        self.data = []\n",
        "        self.index = 0 # index of the next cell to be filled\n",
        "        self.device = device\n",
        "    def append(self, s, a, r, s_, d):\n",
        "        if len(self.data) < self.capacity:\n",
        "            self.data.append(None)\n",
        "        self.data[self.index] = (s, a, r, s_, d)\n",
        "        self.index = (self.index + 1) % self.capacity\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.data, batch_size)\n",
        "        return list(map(lambda x:torch.Tensor(np.array(x)).to(self.device), list(zip(*batch))))\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "_ZsEfOk4AODK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN"
      ],
      "metadata": {
        "id": "5n3x3PfgMpMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implémentons maintenant un DQN avec trois couches de convolutions:"
      ],
      "metadata": {
        "id": "w3txd0PlMq_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AtariCNN(nn.Module):\n",
        "    def __init__(self, in_channels=4, n_actions=6):\n",
        "        super(AtariCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
        "        self.head = nn.Linear(512, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "oMGBkPxMFwEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A votre tour d'implémenter une fonction calculant les Q-values pour un state et jouant greedy à partir de ces Q-values (i.e. retournant l'index de la plus haute Q-value):"
      ],
      "metadata": {
        "id": "1RLgzb7dM4Nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_action(network, state):\n",
        "    device = \"cuda\" if next(network.parameters()).is_cuda else \"cpu\"\n",
        "    with torch.no_grad():\n",
        "        return <your code>"
      ],
      "metadata": {
        "id": "RTxrUyqnEehb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testons votre fonction:"
      ],
      "metadata": {
        "id": "lvzF04ylNLOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state,_ = pong.reset()\n",
        "pong_dqn = AtariCNN()\n",
        "greedy_action(pong_dqn, state)"
      ],
      "metadata": {
        "id": "Da2QTaXdNPRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C) Entraînement"
      ],
      "metadata": {
        "id": "yVbF5-AxNh2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il est temps d'implémenter l'entraînement de notre DQN.\n",
        "Nous allons utiliser:\n",
        "- une exploration epsilon-greedy avec un epsilon qui décroît\n",
        "- un target Q-network pour stabiliser les td errors\n",
        "\n",
        "C'est à vous d'implémenter la méthode `gradient_step` qui met à jour le DQN :)"
      ],
      "metadata": {
        "id": "xtwsDhtdNlvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, config, model):\n",
        "        # Set all parameters\n",
        "        device = \"cuda\" if next(model.parameters()).is_cuda else \"cpu\"\n",
        "        self.nb_actions = config['nb_actions']\n",
        "        self.gamma = config['gamma'] if 'gamma' in config.keys() else 0.95\n",
        "\n",
        "        ## Train every N steps\n",
        "        self.train_freq = config['train_freq'] if 'train_freq' in config.keys() else 1\n",
        "        self.train_warmup = config['train_warmup'] if 'train_warmup' in config.keys() else 1\n",
        "\n",
        "        ## Replay Buffer\n",
        "        ### Number of transitions to sample when training\n",
        "        self.batch_size = config['batch_size'] if 'batch_size' in config.keys() else 100\n",
        "        buffer_size = config['buffer_size'] if 'buffer_size' in config.keys() else int(1e5)\n",
        "        self.memory = ReplayBuffer(buffer_size,device)\n",
        "\n",
        "        ## Epsilon-greedy strategy\n",
        "        self.epsilon_max = config['epsilon_max'] if 'epsilon_max' in config.keys() else 1.\n",
        "        self.epsilon_min = config['epsilon_min'] if 'epsilon_min' in config.keys() else 0.01\n",
        "        self.epsilon_stop = config['epsilon_decay_period'] if 'epsilon_decay_period' in config.keys() else 1000\n",
        "        self.epsilon_delay = config['epsilon_delay_decay'] if 'epsilon_delay_decay' in config.keys() else 20\n",
        "        self.epsilon_step = (self.epsilon_max-self.epsilon_min)/self.epsilon_stop\n",
        "\n",
        "        ## DQN and target DQN\n",
        "        self.model = model\n",
        "        self.target_model = deepcopy(self.model).to(device)\n",
        "\n",
        "        ## Loss / learning rate / optimizer\n",
        "        self.criterion = config['criterion'] if 'criterion' in config.keys() else torch.nn.MSELoss()\n",
        "        lr = config['learning_rate'] if 'learning_rate' in config.keys() else 0.001\n",
        "        self.optimizer = config['optimizer'] if 'optimizer' in config.keys() else torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        ## Number of gradient steps to perform on each batch sampled from the replay buffer\n",
        "        self.nb_gradient_steps = config['gradient_steps'] if 'gradient_steps' in config.keys() else 1\n",
        "\n",
        "        ## Parameter to update the target DQN with a moving average (Polyak average)\n",
        "        self.update_target_tau = config['update_target_tau'] if 'update_target_tau' in config.keys() else 0.005\n",
        "\n",
        "    def gradient_step(self):\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            self.optimizer.zero_grad()\n",
        "            S, A, R, next_S, D = self.memory.sample(self.batch_size)\n",
        "            loss = <your code>\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "    def train(self, env, max_episode):\n",
        "        episode_return = []\n",
        "        episode = 0\n",
        "        episode_cum_reward = 0\n",
        "        state, _ = env.reset()\n",
        "        epsilon = self.epsilon_max\n",
        "        step = 0\n",
        "        while episode < max_episode:\n",
        "            # update epsilon\n",
        "            if step > self.epsilon_delay:\n",
        "                epsilon = max(self.epsilon_min, epsilon-self.epsilon_step)\n",
        "            # select epsilon-greedy action\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = greedy_action(self.model, state)\n",
        "\n",
        "            # step\n",
        "            next_state, reward, done, trunc, _ = env.step(action)\n",
        "            # record transition in replay buffer\n",
        "            self.memory.append(state, action, reward, next_state, done)\n",
        "            episode_cum_reward += reward\n",
        "\n",
        "            # train\n",
        "            if step > self.train_warmup and step % self.train_freq == 0:\n",
        "                for _ in range(self.nb_gradient_steps):\n",
        "                    self.gradient_step()\n",
        "\n",
        "                # update target network with Polyak average\n",
        "                target_state_dict = self.target_model.state_dict()\n",
        "                model_state_dict = self.model.state_dict()\n",
        "                tau = self.update_target_tau\n",
        "                for key in model_state_dict:\n",
        "                    target_state_dict[key] = tau*model_state_dict[key] + (1-tau)*target_state_dict[key]\n",
        "                self.target_model.load_state_dict(target_state_dict)\n",
        "\n",
        "            # next transition\n",
        "            step += 1\n",
        "            if done or trunc:\n",
        "                episode += 1\n",
        "                print(\"Episode \", '{:3d}'.format(episode),\n",
        "                      \", steps \", '{:3d}'.format(step),\n",
        "                      \", epsilon \", '{:6.2f}'.format(epsilon),\n",
        "                      \", batch size \", '{:5d}'.format(len(self.memory)),\n",
        "                      \", episode return \", '{:4.1f}'.format(episode_cum_reward),\n",
        "                      sep='')\n",
        "                state, _ = env.reset()\n",
        "                episode_return.append(episode_cum_reward)\n",
        "                episode_cum_reward = 0\n",
        "            else:\n",
        "                state = next_state\n",
        "        return episode_return"
      ],
      "metadata": {
        "id": "ksmIGcNfR2S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C'est parti pour lancer l'entraînement de notre DQN !"
      ],
      "metadata": {
        "id": "8d4SLQftvdCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare network\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "state_dim = pong.observation_space.shape[0]\n",
        "n_action = pong.action_space.n\n",
        "nb_neurons=24\n",
        "\n",
        "DQN = AtariCNN()\n",
        "\n",
        "# DQN config\n",
        "config = {'nb_actions': pong.action_space.n,\n",
        "          'train_warmup': 10000,\n",
        "          'train_freq': 10,\n",
        "          'gradient_steps': 1,\n",
        "          'learning_rate': 0.001,\n",
        "          'gamma': 0.95,\n",
        "          'buffer_size': 100000,\n",
        "          'epsilon_min': 0.1,\n",
        "          'epsilon_max': 1.,\n",
        "          'epsilon_decay_period': 1000000,\n",
        "          'epsilon_delay_decay': 10000,\n",
        "          'batch_size': 64}\n",
        "\n",
        "# Train agent\n",
        "agent = DQNAgent(config, DQN)\n",
        "scores = agent.train(pong, 200)\n",
        "plt.plot(scores)"
      ],
      "metadata": {
        "id": "wOcRY7rsEoKz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}